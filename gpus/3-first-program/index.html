<!doctype html><html lang=en><head><meta content="IE=edge"http-equiv=X-UA-Compatible><meta content="text/html; charset=utf-8"http-equiv=content-type><meta content="width=device-width,initial-scale=1.0,maximum-scale=1,viewport-fit=cover"name=viewport><title>//TODO(rose) | GPUs, Part 3: An Example Program</title><link href=https://blog.krx.sh/theme.css rel=stylesheet><body><div id=main><div class=sidebar><div class=sidebar-inner><h1>//TODO(rose)</h1><p><a href=/> homepage </a> <br> <a href=/about/> about me </a> <br> <a href=https://github.com/rosehuds>github</a></div></div><div class=content><article><h1>GPUs, Part 3: An Example Program</h1><p>Now that we've seen some of the theory behind GPUs, it's time to see how it works in practice. How can a lowly mortal like me speak to a Titan (heh)? What do we need to know before going in? Comes with free wgpu tutorial.</p><span id=continue-reading></span><h2 id=shaders-and-the-opengl-model><a aria-label="Anchor link for: shaders-and-the-opengl-model"class=anchor-link href=#shaders-and-the-opengl-model>#</a> Shaders and the OpenGL model</h2><p>There are some differing opinions on what to call a program that runs on a GPU. It seems that people from graphics land call them shaders, because they are generally used to calculate the colour or light level of an object. There are a few types of shader and each has its own constraints, but the most generic one and the one that we are interested in is the compute shader. In compute land, they are called kernels, but there's already a thing called kernel and "shader" sounds cooler to me so we're going with that.<p>Since there are so many types of GPU, programs tend to be stored in some intermediate format and compiled to target a certain architecture at runtime. This compilation is performed by the driver, which allows it to be device-specific. We can see this reflected in OpenGL, where the API ingests shaders written in GLSL. This means there is no ahead-of-time compilation step needed if you're happy to write your shaders in GLSL. Vulkan takes a slightly different approach by ingesting SPIR-V which is a binary format, one step below the human-friendly GLSL, but still above the hardware in terms of abstraction.<h3 id=gl-globalinvocationid><a aria-label="Anchor link for: gl-globalinvocationid"class=anchor-link href=#gl-globalinvocationid>#</a> <code>gl_GlobalInvocationID</code></h3><p>Things get a little funky in this section, so hold on tight. Work done by compute shaders tends to be intuitively divisible into chunks which exist in some n-dimensional space. For example, a shader which modifies every pixel of an 800x600 image in some way can have its work divided into 800 * 600 = 480000 chunks, one for each pixel. These chunks can be named using the coordinates of the pixel they work on, and so the chunks exist in 2-dimensional space.<p>In this case, the programmer can write a shader that operates on one pixel, and in OpenGL terms, use <code>glDispatchCompute(800, 600, 1)</code> to run the compute shader. The arguments to this function specify how many instances of the shader will be run in each dimension of x, y, and z. Here, z is 1 because the image is 2-dimensional. This size is called <code>gl_NumWorkGroups</code>. But wait, I hear you say, why is one unit of work called a work group?<p>Enter the funk, stage left. Recall that <code>gl_NumWorkGroups</code> tells us how many <em>instances of the shader</em> will be run. Well, the shader itself has a <code>gl_WorkGroupSize</code>, which is another 3-dimensional size that represents how many times the code will be run, per instance of the shader.<p>This means that your code runs inside a... 6-orthotope? ðŸ¤”<p>I can't think of a great example for this, but suppose the image processing shader actually operated on 4x4 groups of pixels. This is when you could use a 4x4x1 <code>gl_WorkGroupSize</code> and a 200x150x1 <code>gl_NumWorkGroups</code>. The general idea is that communication will be faster between threads within a work group than it will be across work groups.<p>All of these <code>gl_*</code> variables are accessible from within the shader, along with a couple that I haven't mentioned. <code>gl_LocalInvocationID</code> is a 3D representation of which unit within a work group is currently being run, and <code>gl_WorkGroupID</code> represents which work group within the entire dispatch is being run. The most important one is probably <code>gl_GlobalInvocationID</code>, which is a 3D identifier that is unique across all invocations of your code. It is defined as <code>gl_WorkGroupID * gl_WorkGroupSize + gl_LocalInvocationID</code>, which has the nice effect of meaning that you can use it as an index into some data. In the image example, the shader code could use <code>gl_GlobalInvocationID</code> directly to access its respective pixel from the image. Nice!<h2 id=wgpu><a aria-label="Anchor link for: wgpu"class=anchor-link href=#wgpu>#</a> wgpu</h2><p>APIs for working with GPUs are scary, they have a lot of long words, and worst of all are usually designed for C programmers. wgpu is an exception to at least one of these rules, because it provides a relatively nice Rust interface while retaining design choices of things like Vulkan that allow you relatively close access to GPU (or driver) behaviour. It also supports multiple backends, so you can write code in terms of wgpu and run it on systems supporting Vulkan, Metal, and a couple of versions of DirectX, among others.<p>Before anything can be done with wgpu, you must start with an <code>Instance</code>. While creating this object, a backend is chosen. Then, this instance can be used to create an <code>Adapter</code>, which is like a physical device. At this point, you can specify whether to use a low power adapter or a high performance one, and provide information about window systems if you intend to render to a window. The next link in the chain is called <code>Device</code>, which is more like a session on a device than the device itself. Poor naming, in my opinion. <code>Device</code> is part of a package deal with <code>Queue</code> - the former allows control over resources and the latter allows control over actions.<p>When shaders are running, they need to have access to some objects to work on. There are a few types of these, but the simplest is the buffer, which is a contiguous region in memory. Objects like these are the main way for GPUs to communicate with the rest of a computer, and they can be constructed using the methods on <code>Device</code>.<p>To start with, these methods can be used to create a <code>BindGroupLayout</code> which is a template for a group of objects and their properties. For example, a <code>BindGroupLayout</code> might describe two read-only buffers which are visible to compute shaders and are at least 4096 bytes in size. Then, a <code>BindGroup</code> can be created which has this layout, by naming two buffers with those properties.<p>To use a compute shader, you need a "compute pipeline", which is an object that holds the shader and some bind group layouts. Telling a GPU to run a shader (after you've done all the setup) is done using command buffers, which are built using command encoders. These commands have semantics like "run this compute pipeline using these bind groups", and such a command also contains a size in x, y, and z, which are equivalent to the arguments we saw being passed to <code>glDispatchCompute</code> earlier. The command buffer can be submitted to the GPU via the <code>Queue</code>.<h2 id=like-hello-world-but-worse><a aria-label="Anchor link for: like-hello-world-but-worse"class=anchor-link href=#like-hello-world-but-worse>#</a> Like "Hello, World!" but worse</h2><p><em>The code for this section is available <a href=https://github.com/rosehuds/gpu-playground/tree/main/wgpu>here</a>. If you clone the repository, note that this link points to a specific commit, and not the main branch. I won't write any of the CPU-side code in here, because it's basically a translation of the previous section into Rust. All you need to know is it runs the shader with a size of (64, 1, 1), passing it a buffer of 512 * 4 bytes, and finally prints the contents of the buffer.</em><p>Now that we know how to run a compute shader, all that's left is knowing how to write one. For this, I use rust-gpu. Diving right in, here's an example shader:<pre class=language-rust data-lang=rust style=background-color:#2b303b;color:#c0c5ce;><code class=language-rust data-lang=rust><span>#![</span><span style=color:#bf616a;>cfg_attr</span><span>(
</span><span>    target_arch = "</span><span style=color:#a3be8c;>spirv</span><span>",
</span><span>    </span><span style=color:#bf616a;>feature</span><span>(register_attr),
</span><span>    </span><span style=color:#bf616a;>register_attr</span><span>(spirv),
</span><span>    no_std
</span><span>)]
</span><span>
</span><span style=color:#b48ead;>extern crate</span><span> spirv_std;
</span><span>
</span><span style=color:#b48ead;>use </span><span>spirv_std::glam::UVec3;
</span><span>
</span><span>#[</span><span style=color:#bf616a;>spirv</span><span>(</span><span style=color:#bf616a;>compute</span><span>(</span><span style=color:#bf616a;>threads</span><span>(8, 1, 1)))]
</span><span style=color:#b48ead;>pub fn </span><span style=color:#8fa1b3;>main_cs</span><span>(
</span><span>    #[spirv(</span><span style=color:#bf616a;>global_invocation_id</span><span>)]
</span><span>    </span><span style=color:#bf616a;>gid</span><span>: UVec3,
</span><span>    #[spirv(</span><span style=color:#bf616a;>storage_buffer</span><span>, </span><span style=color:#bf616a;>descriptor_set</span><span> = 0, </span><span style=color:#bf616a;>binding</span><span> = 0)]
</span><span>    </span><span style=color:#bf616a;>buffer</span><span>: &</span><span style=color:#b48ead;>mut</span><span> [</span><span style=color:#b48ead;>u32</span><span>],
</span><span>) {
</span><span>    buffer[gid.x as </span><span style=color:#b48ead;>usize</span><span>] = </span><span style=color:#d08770;>1</span><span>;
</span><span>}
</span></code></pre><p>At the beginning, we have some magical incantations probably copied from the rust-gpu repository. I think they allow the <code>#[spirv]</code> attribute to work in the <code>main_cs</code> function's arguments. Then, we import <code>UVec3</code> from glam, a maths library which comes with <code>spirv_std</code>. This type is effectively <code>(u32, u32, u32)</code> and it is the type of the <code>gid</code> argument, which is the same as <code>gl_GlobalInvocationID</code>.<p>On the main function there is an attribute telling the compiler that this is the entry point of a compute shader whose work group size is (8, 1, 1), and in the arguments we see the global ID and a buffer of <code>u32</code>s, which is the first binding in descriptor set 0. I'm pretty sure "descriptor set" is the Vulkan term for bind group, which would make sense because it is possible to pass multiple bind groups to a shader.<p>Overall, the shader sets 8 elements of the buffer to <code>1</code>, and the runner runs it 64 times. Only the x dimension is used because the buffer is 1-dimensional. By running the program, we can see that it does as expected and prints out a long list of ones (512 of them), all of which came from the GPU!<h2 id=wow><a aria-label="Anchor link for: wow"class=anchor-link href=#wow>#</a> Wow</h2><p>Phew! That was a lot of strange words. Arguably, we didn't achieve very much - all we did was set some bits to new values, but isn't that all that computers do anyway? I'd say that we did achieve a lot. We started out knowing a couple of concepts about predication or whatever and ended up running a whole entire program on a processor widely regarded as "for wizards" (not a real quote). Who's the wizard now? You are! If you got this far, I'm proud of you, and you should be proud of yourself. Go and make yourself a cup of tea or something as a reward. When you get back, there might even be some more wizardry to look at ðŸ˜‰<hr><div class=prev-next><a href=https://blog.krx.sh/gpus/2-architecture/> Previous</a> Â |Â  Next</div></article></div></div>